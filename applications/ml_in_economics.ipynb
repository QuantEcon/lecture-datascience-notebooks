{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning in economics\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- applications/regression  \n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the ways that economists are using machine learning in\n",
    "  academic research  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.style.use('tableau-colorblind10')\n",
    "#plt.style.use('Solarize_Light2')\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning is increasingly being utilized in economic\n",
    "research. Here, we discuss three main ways that economists are\n",
    "currently using machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Policy\n",
    "\n",
    "The most obvious situation where machine learning methods can be used\n",
    "in economic research or policy making is when the goal is prediction\n",
    "per se. Most empirical economic research focuses on questions of\n",
    "causuality. However, there are interesting economic situations and\n",
    "policy questions where the goal is prediction instead of causality.\n",
    "[[KLMO15]](#kleinberg2015) is a short paper making this point.\n",
    "\n",
    "> Consider two toy examples. One policymaker facing a drought must\n",
    "decide whether to invest in a rain dance to increase the chance\n",
    "of rain.  Another seeing clouds must decide whether to take an\n",
    "umbrella to work to avoid getting wet on the way home. Both\n",
    "decisions could benefit from an empirical study of rain. But each\n",
    "has differ-ent requirements of the estimator. One requires\n",
    "causality: Do rain dances cause rain? The other does not, needing\n",
    "only prediction: Is the chance of rain high enough to merit an\n",
    "umbrella?  We often focus on rain dance–like policy problems. But\n",
    "there are also many umbrella-like policy problems.  Not only are\n",
    "these prediction problems neglected, machine learning can help\n",
    "us solve them more effectively.  [[KLMO15]](#kleinberg2015)\n",
    "\n",
    "\n",
    "An example that they give is the allocation of joint replacements for\n",
    "osteoarthritis in elderly patients. Joint replacements are costly,\n",
    "both monetarily and in terms of potentially painful recovery from\n",
    "surgery. Joint replacements may not be worthwhile for patients who do\n",
    "not live long enough afterward to enjoy the\n",
    "benefits. [[KLMO15]](#kleinberg2015) use machine learning methods to\n",
    "predict mortality, and argue that not performing joint replacements\n",
    "for people with the highest predicted mortality risk could lead to\n",
    "sizeable benefits.\n",
    "\n",
    "Other situations where improved prediction could improve economic\n",
    "policy include:\n",
    "\n",
    "- Targeting safety or health inspections  \n",
    "- Predicting highest risk youth for targeting interventions  \n",
    "- Improved risk scoring in insurance markets to reduce adverse\n",
    "  selection  \n",
    "- Improved credit scoring to better allocate credit  \n",
    "- Predicting the risk someone accused of a crime does not show up for\n",
    "  trial to help decide whether to offer bail [[KLL+17]](recidivism.ipynb#kleinberg2017)  \n",
    "\n",
    "\n",
    "We investigated one such prediction policy problem in\n",
    "applications/recidivism ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of Nuisance Functions\n",
    "\n",
    "Most empirical economic studies are interested in a single low\n",
    "dimensional parameter, but to estimate this parameter, many additional\n",
    "“nuisance” parameters might have to be estimated. In most regression\n",
    "models, there is one explanatory variable of interest, and the\n",
    "researcher’s main goal is to estimate the coefficient on this\n",
    "variable. To estimate this coefficient consistently, often many other\n",
    "variables are included in the regression to avoid omitted variables\n",
    "bias. However, the choice of which other variables to include and\n",
    "their functional forms is often somewhat arbitrary.\n",
    "One promising idea is to use machine learning methods to let the data\n",
    "decide what control variables to include and how. Care must be\n",
    "taken when doing so though because the flexibility and complexity\n",
    "that make machine learning so good at prediction also pose\n",
    "challenges for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially Linear Regression\n",
    "\n",
    "To be more concrete, consider a regression model.  We have some\n",
    "regressor of interest, $ d $, and we want to estimate the effect of $ d $\n",
    "on $ y $. We have a rich enough set of controls, $ x $ that we are willing to\n",
    "believe that $ E[\\epsilon|d,x] = 0 $ . $ d_i $ and $ y_i $ are scalars, while\n",
    "$ x_i $ is a vector. We are not interested in $ x $ per se, but we need to\n",
    "include it to avoid omitted variable bias. Suppose the true model\n",
    "generating the data is\n",
    "\n",
    "$$\n",
    "y = \\theta d + f(x) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $ f(x) $ is some unknown function. This is called a\n",
    "partially linear model it is linear in $ d $, but not in\n",
    "$ x $ .\n",
    "\n",
    "A typical applied econometric approach for this model would\n",
    "be to choose some transfrom of $ x $, say $ X = T(x) $, where $ X $\n",
    "could be some subset of $ x $ , perhaps along with interactions, powers, and\n",
    "so on. Then estimate a linear regression\n",
    "\n",
    "$$\n",
    "y = \\theta d + X'\\beta + e\n",
    "$$\n",
    "\n",
    "and then perhaps also report results for a handful of different\n",
    "choices of $ T(x) $ .\n",
    "\n",
    "Some downsides to the typical applied econometric practice\n",
    "include:\n",
    "\n",
    "- The choice of T is arbitrary, which opens the door to specification\n",
    "  searching and p-hacking.  \n",
    "- If $ x $ is high dimensional, and $ X $ is low dimensional, a poor\n",
    "  choice will lead to omitted variable bias. Even if $ x $ is low\n",
    "  dimensional, if $ f(x) $ is poorly approximated by $ X'\\beta $\n",
    "  , there will be omitted variable bias.  \n",
    "\n",
    "\n",
    "In some sense, machine learning can be thought of as a way to\n",
    "choose $ T $ is an automated and data-driven way. There will be\n",
    "still be a choice of machine learning method and often tuning\n",
    "parameters for that method, so some arbitrary decisions\n",
    "remain. Hopefully though these decisions have less impact.\n",
    "\n",
    "Economic researchers typically want not just an estimate of\n",
    "$ \\theta $, $ \\hat{\\theta} $, they also want to know that\n",
    "$ \\hat{\\theta} $ has good statistical properties (it should at\n",
    "least be consistent), and they want some way to quantify how uncertain is\n",
    "$ \\hat{\\theta} $ (i.e. they want a standard error). The complexity\n",
    "of machine learning methods makes their statistical properties\n",
    "difficult to understand. If we want $ \\hat{\\theta} $ to have\n",
    "known and good statistical properties we must make sure we use machine\n",
    "learning methods in the correct way.  A procedure to estimate\n",
    "$ \\theta $ in the partially linear model is as follows:\n",
    "\n",
    "1. Predict $ y $ and $ d $ from $ x $ using any machine\n",
    "  learning method with “cross-fitting”  \n",
    "  - Partition the data in $ k $ subsets  \n",
    "  - For the $ j $ th subset, train models to predict $ y $ and $ d $\n",
    "    using the other $ k-1 $ subsets. Denote the predictions from\n",
    "    these models as $ p^y_{-j}(x) $ and  $ p^d_{-j}(x) $  \n",
    "  - For $ y_i $ in the $ j $ -th subset use the other\n",
    "    $ k-1 $ subsets to predict $ \\hat{y}_i = p^y_{-j(i)}(x_i) $  \n",
    "1. Partial out $ x $ : let $ \\tilde{y}_i = y_i - \\hat{y}_i $\n",
    "  and $ \\tilde{d}_i = d_i - \\hat{d}_i $  \n",
    "1. Regress $ \\tilde{y}_i $ on $ \\tilde{d}_i $, let\n",
    "  $ \\hat{\\theta} $ be the estimated coefficient for\n",
    "  $ \\tilde{d}_i $ . $ \\hat{\\theta} $ is consistent,\n",
    "  asymptotically normal, and has the usual standard error (i.e. the\n",
    "  standard error given by statsmodels is correct)  \n",
    "\n",
    "\n",
    "Some remarks\n",
    "\n",
    "- This procedure gives a $ \\hat{\\theta} $ that has the same\n",
    "  asymptotic distribution as what we would get if we knew the true\n",
    "  $ f(x) $ . In statistics we call this an oracle property,\n",
    "  because it is as if an all knowing oracle told us $ f(x) $  \n",
    "- There are some technical conditions on the data generating\n",
    "  process and machine learning estimator needed for this procedure to\n",
    "  work. We will not worry about them here. See\n",
    "  [[CCD+18]](regression.ipynb#chernozhukov2018) for details.  \n",
    "\n",
    "\n",
    "Here is code implementing the above idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import statsmodels as sm\n",
    "\n",
    "def partial_linear(y, d, X, yestimator, destimator, folds=3):\n",
    "    \"\"\"Estimate the partially linear model y = d*C + f(x) + e\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        vector of outcomes\n",
    "    d : array_like\n",
    "        vector or matrix of regressors of interest\n",
    "    X : array_like\n",
    "        matrix of controls\n",
    "    mlestimate : Estimator object for partialling out X. Must have ‘fit’\n",
    "        and ‘predict’ methods.\n",
    "    folds : int\n",
    "        Number of folds for cross-fitting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ols : statsmodels regression results containing estimate of coefficient on d.\n",
    "    yhat : cross-fitted predictions of y\n",
    "    dhat : cross-fitted predictions of d\n",
    "    \"\"\"\n",
    "\n",
    "    # we want predicted probabilities if y or d is discrete\n",
    "    ymethod = \"predict\" if False==getattr(yestimator, \"predict_proba\",False) else \"predict_proba\"\n",
    "    dmethod = \"predict\" if False==getattr(destimator, \"predict_proba\",False) else \"predict_proba\"\n",
    "    # get the predictions\n",
    "    yhat = cross_val_predict(yestimator,X,y,cv=folds,method=ymethod)\n",
    "    dhat = cross_val_predict(destimator,X,d,cv=folds,method=dmethod)\n",
    "    ey = np.array(y - yhat)\n",
    "    ed = np.array(d - dhat)\n",
    "    ols = sm.regression.linear_model.OLS(ey,ed).fit(cov_type='HC0')\n",
    "\n",
    "    return(ols, yhat, dhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Gender Wage Gap\n",
    "\n",
    "Okay, enough theory, let’s look at an application. Policy makers have\n",
    "long been concerned with the gender wage gap. We will examine the\n",
    "gender wage gap using data from the 2018 Current Population Survey (CPS) in\n",
    "the US. In particular, we will use the version of the [CPS provided by\n",
    "the NBER.](https://www.nber.org/cps/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "# Download CPS data\n",
    "cpsall = pd.read_stata(\"https://www.nber.org/morg/annual/morg18.dta\")\n",
    "\n",
    "# take subset of data just to reduct computation time\n",
    "# FIXME: set rng seed to make reproducible\n",
    "cps = cpsall.sample(30000, replace=False)\n",
    "display(cps.head())\n",
    "cps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable “earnwke” records weekly earnings. There are two\n",
    "variables about hours of work. “uhours” is usual hours worked per\n",
    "week, and “hourslw” is hours worked last week. We will try using each\n",
    "measure of hours to construct the wage.  Let’s estimate the\n",
    "unconditional gender earnings and wage gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "cps[\"female\"] = (cps.sex==2)\n",
    "cps[\"log_earn\"] = np.log(cps.earnwke)\n",
    "cps[\"log_earn\"][np.isinf(cps.log_earn)] = np.nan\n",
    "cps[\"log_uhours\"] = np.log(cps.uhourse)\n",
    "cps[\"log_uhours\"][np.isinf(cps.log_uhours)] = np.nan\n",
    "cps[\"log_hourslw\"] = np.log(cps.hourslw)\n",
    "cps[\"log_hourslw\"][np.isinf(cps.log_hourslw)] = np.nan\n",
    "cps[\"log_wageu\"] = cps.log_earn - cps.log_uhours\n",
    "cps[\"log_wagelw\"] = cps.log_earn - cps.log_hourslw\n",
    "\n",
    "\n",
    "lm = list()\n",
    "lm.append(smf.ols(formula=\"log_earn ~ female\", data=cps,\n",
    "                  missing=\"drop\").fit(cov_type='HC0'))\n",
    "lm.append( smf.ols(formula=\"log_wageu ~ female\", data=cps,\n",
    "                   missing=\"drop\").fit(cov_type='HC0'))\n",
    "lm.append(smf.ols(formula=\"log_wagelw ~ female\", data=cps,\n",
    "                  missing=\"drop\").fit(cov_type='HC0'))\n",
    "lm.append(smf.ols(formula=\"log_earn ~ female + log_hourslw + log_uhours\", data=cps,\n",
    "                  missing=\"drop\").fit(cov_type='HC0'))\n",
    "\n",
    "summary_col(lm, stars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal Pay for Equal Work?\n",
    "\n",
    "A common slogan is equal pay for equal work. One way to interpret this\n",
    "is conditional on observable job and work characteristics, there\n",
    "should be no gender wage gap.\n",
    "\n",
    "Lack of a gender wage gap conditional on observables is a necessary\n",
    "but not sufficient condition for equality. Some of the differences\n",
    "in observables (like hours in the 4th column of the above table)\n",
    "might themselves be a result of societal norms and/or\n",
    "discrimination, and we may not want to hold them constant.\n",
    "\n",
    "Nonetheless, let’s examine whether there is a gender wage gap\n",
    "conditional on all worker and job characteristics. We want to ensure\n",
    "that we control for worker and job characteristics as flexibly as\n",
    "possible, so we will use the partially linear model described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "# Prepare data\n",
    "fmla  = \"log_earn + female ~ log_uhours + log_hourslw + age + I(age**2) + C(race) + C(cbsafips) + C(smsastat) + C(grade92) + C(unionmme) + C(unioncov) + C(ind02) + C(occ2012)\"\n",
    "yd, X = dmatrices(fmla,cps)\n",
    "female = yd[:,1]\n",
    "logearn = yd[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# select regularization parameter\n",
    "alphas = np.exp(np.linspace(-2.., -12., 25))\n",
    "lassoy = linear_model.LassoCV(cv=6, alphas=alphas, max_iter=5000).fit(X,logearn)\n",
    "lassod = linear_model.LassoCV(cv=6, alphas=alphas, max_iter=5000).fit(X,female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "def plotlassocv(l, ax) :\n",
    "    alphas = l.alphas_\n",
    "    mse = l.mse_path_.mean(axis=1)\n",
    "    std_error = l.mse_path_.std(axis=1)\n",
    "    ax.plot(alphas,mse)\n",
    "    ax.fill_between(alphas, mse + std_error, mse - std_error, alpha=0.2)\n",
    "\n",
    "    ax.set_ylabel('MSE +/- std error')\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_xlim([alphas[0], alphas[-1]])\n",
    "    ax.set_xscale(\"log\")\n",
    "    return(ax)\n",
    "\n",
    "ax[0] = plotlassocv(lassoy,ax[0])\n",
    "ax[0].set_title(\"MSE for log(earn)\")\n",
    "ax[1] = plotlassocv(lassod,ax[1])\n",
    "ax[1].set_title(\"MSE for female\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# there are theoretical reasons to choose a smaller regularization\n",
    "# than the one that minimizes cv. BUT THIS WAY OF CHOOSING IS ARBITRARY AND MAYBE WRONG\n",
    "def pickalpha(lassocv) :\n",
    "    imin = np.argmin(lassocv.mse_path_.mean(axis=1))\n",
    "    msemin = lassocv.mse_path_.mean(axis=1)[imin]\n",
    "    se = lassocv.mse_path_.std(axis=1)[imin]\n",
    "    alpha= min([alpha for (alpha, mse) in zip(lassocv.alphas_, lassocv.mse_path_.mean(axis=1)) if mse<msemin+se])\n",
    "    return(alpha)\n",
    "\n",
    "alphay = pickalpha(lassoy)\n",
    "alphad = pickalpha(lassod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# show results\n",
    "pl_lasso = partial_linear(logearn, female, X,\n",
    "                          linear_model.Lasso(alpha=lassoy.alpha_),\n",
    "                          linear_model.Lasso(alpha=lassod.alpha_))\n",
    "pl_lasso[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Visualize predictions\n",
    "def plotpredictions(pl) :\n",
    "    df = pd.DataFrame({\"logearn\":logearn,\n",
    "                       \"predicted\":pl[1],\n",
    "                       \"female\":female,\n",
    "                       \"P(female|x)\":pl[2]})\n",
    "    sns.pairplot(df, vars=[\"logearn\",\"predicted\"], hue=\"female\")\n",
    "    plt.title(\"Observed and predicted log(earnings)\")\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(df.predicted, df.logearn-df.predicted, hue=df.female)\n",
    "    plt.title(\"Prediction Errors\")\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(pl[2][female==0], hist = True, kde = False,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3},\n",
    "                 label = \"Male\")\n",
    "    sns.distplot(pl[2][female==1], hist = True, kde = False,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3},\n",
    "                 label = \"Female\")\n",
    "    plt.title('P(female|x)')\n",
    "plotpredictions(pl_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "from sklearn import preprocessing, pipeline, model_selection\n",
    "\n",
    "nnp = pipeline.Pipeline(steps=[\n",
    "    (\"scaling\", preprocessing.StandardScaler()),\n",
    "    (\"nn\", neural_network.MLPRegressor((50,), activation=\"logistic\",\n",
    "                                       verbose=False, solver=\"adam\",\n",
    "                                       max_iter=400, early_stopping=True,\n",
    "                                       validation_fraction=0.15))\n",
    "])\n",
    "\n",
    "nndcv = model_selection.GridSearchCV(estimator=nnp, scoring= 'neg_mean_squared_error', cv=4,\n",
    "                                     param_grid = {'nn__alpha': np.exp(np.linspace(-5,5, 10))},\n",
    "                                     return_train_score=True, verbose=True, refit=False,\n",
    "                                     iid=True).fit(X,female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "nnycv = model_selection.GridSearchCV(estimator=nnp, scoring= 'neg_mean_squared_error', cv=4,\n",
    "                                     param_grid = {'nn__alpha': np.exp(np.linspace(-5,5, 10))},\n",
    "                                     return_train_score=True, verbose=True, refit=False,\n",
    "                                     iid=True).fit(X,logearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "def plotgridcv(g, ax) :\n",
    "    alphas = g.cv_results_[\"param_nn__alpha\"].data.astype(float)\n",
    "    mse = -g.cv_results_[\"mean_test_score\"]\n",
    "    std_error = g.cv_results_[\"std_test_score\"]\n",
    "    ax.plot(alphas,mse)\n",
    "    ax.fill_between(alphas, mse+std_error, mse-std_error, alpha=0.2)\n",
    "\n",
    "    ax.set_ylabel('MSE +/- std error')\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_xlim([alphas[0], alphas[-1]])\n",
    "    ax.set_xscale(\"log\")\n",
    "    return(ax)\n",
    "\n",
    "ax[0] = plotgridcv(nnycv,ax[0])\n",
    "ax[0].set_title(\"MSE for log(earn)\")\n",
    "ax[1] = plotgridcv(nndcv,ax[1])\n",
    "ax[1].set_title(\"MSE for female\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# there are theoretical reasons to choose a smaller regularization\n",
    "# than the one that minimizes cv. BUT THIS WAY OF CHOOSING IS ARBITRARY AND MAYBE WRONG\n",
    "def pickalphagridcv(g) :\n",
    "    alphas = g.cv_results_[\"param_nn__alpha\"].data\n",
    "    mses = g.cv_results_[\"mean_test_score\"]\n",
    "    imin = np.argmin(mses)\n",
    "    msemin = mses[imin]\n",
    "    se = g.cv_results_[\"std_test_score\"][imin]\n",
    "    alpha= min([alpha for (alpha, mse) in zip(alphas, mses) if mse<msemin+se])\n",
    "    return(alpha)\n",
    "\n",
    "alphaynn = pickalphagridcv(nnycv)\n",
    "alphadnn = pickalphagridcv(nndcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# show results\n",
    "nny = nnp\n",
    "nny.set_params(nn__alpha = alphaynn)\n",
    "nnd = nnp\n",
    "nnd.set_params(nn__alpha = alphadnn)\n",
    "pl_nn = partial_linear(logearn, female, X,\n",
    "                       nny, nnd)\n",
    "pl_nn[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plotpredictions(pl_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "summary_col([pl_lasso[0], pl_nn[0]], model_names=[\"Lasso\", \"Neural Network\"] ,stars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other applications\n",
    "\n",
    "There are many other situations where machine learning can be used to\n",
    "estimate nuisance functions. The partially linear model can easily be\n",
    "extended to situations where the regressor of interest, $ d $ , is\n",
    "endogenous and instruments are available. See [[CCD+18]](regression.ipynb#chernozhukov2018)\n",
    "for details and additional examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogeneous Effects\n",
    "\n",
    "A third area where economists are using machine learning is to\n",
    "estimate heterogeneous effects.\n",
    "\n",
    "Some important papers in this area are [[AI16]](#athey2016b) ,\n",
    "[[WA18]](#wager2018) , and [[CDDFV18]](#cddf2018) .\n",
    "\n",
    "We will explore this is more depth in applications/heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id='alatas2011'></a>\n",
    "\\[ACE+11\\] Vivi Alatas, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. Program keluarga harapan : impact evaluation of indonesia’s pilot household conditional cash transfer program. Technical Report, World Bank, 2011. URL: [http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program](http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program).\n",
    "\n",
    "<a id='athey2016b'></a>\n",
    "\\[AI16\\] Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. *Proceedings of the National Academy of Sciences*, 113(27):7353–7360, 2016. URL: [http://www.pnas.org/content/113/27/7353](http://www.pnas.org/content/113/27/7353), [arXiv:http://www.pnas.org/content/113/27/7353.full.pdf](https://arxiv.org/abs/http://www.pnas.org/content/113/27/7353.full.pdf), [doi:10.1073/pnas.1510489113](https://doi.org/10.1073/pnas.1510489113).\n",
    "\n",
    "<a id='chernozhukov2018'></a>\n",
    "\\[CCD+18\\] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1):C1–C68, 2018. URL: [https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097](https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097), [arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097](https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097), [doi:10.1111/ectj.12097](https://doi.org/10.1111/ectj.12097).\n",
    "\n",
    "<a id='cddf2018'></a>\n",
    "\\[CDDFV18\\] Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iván Fernández-Val. Generic machine learning inference on heterogenous treatment effects in randomized experimentsxo. Working Paper 24678, National Bureau of Economic Research, June 2018. URL: [http://www.nber.org/papers/w24678](http://www.nber.org/papers/w24678), [doi:10.3386/w24678](https://doi.org/10.3386/w24678).\n",
    "\n",
    "<a id='kleinberg2017'></a>\n",
    "\\[KLL+17\\] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human Decisions and Machine Predictions*. *The Quarterly Journal of Economics*, 133(1):237–293, 08 2017. URL: [https://dx.doi.org/10.1093/qje/qjx032](https://dx.doi.org/10.1093/qje/qjx032), [arXiv:http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf](https://arxiv.org/abs/http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf), [doi:10.1093/qje/qjx032](https://doi.org/10.1093/qje/qjx032).\n",
    "\n",
    "<a id='kleinberg2015'></a>\n",
    "\\[KLMO15\\] Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer. Prediction policy problems. *American Economic Review*, 105(5):491–95, May 2015. URL: [http://www.aeaweb.org/articles?id=10.1257/aer.p20151023](http://www.aeaweb.org/articles?id=10.1257/aer.p20151023), [doi:10.1257/aer.p20151023](https://doi.org/10.1257/aer.p20151023).\n",
    "\n",
    "<a id='triyana2016'></a>\n",
    "\\[Tri16\\] Margaret Triyana. Do health care providers respond to demand-side incentives? evidence from indonesia. *American Economic Journal: Economic Policy*, 8(4):255–88, November 2016. URL: [http://www.aeaweb.org/articles?id=10.1257/pol.20140048](http://www.aeaweb.org/articles?id=10.1257/pol.20140048), [doi:10.1257/pol.20140048](https://doi.org/10.1257/pol.20140048).\n",
    "\n",
    "<a id='wager2018'></a>\n",
    "\\[WA18\\] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. *Journal of the American Statistical Association*, 0(0):1–15, 2018. URL: [https://doi.org/10.1080/01621459.2017.1319839](https://doi.org/10.1080/01621459.2017.1319839), [arXiv:https://doi.org/10.1080/01621459.2017.1319839](https://arxiv.org/abs/https://doi.org/10.1080/01621459.2017.1319839), [doi:10.1080/01621459.2017.1319839](https://doi.org/10.1080/01621459.2017.1319839)."
   ]
  }
 ],
 "metadata": {
  "filename": "ml_in_economics.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Machine learning in economics"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}