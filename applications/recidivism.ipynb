{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Recidivism\n",
    "\n",
    "*This material owes a special thanks to Arnav Snood who helped replicate pieces of the original Pro\n",
    "Publica analysis, developed regression models, and explored various pieces of the data*\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- [matplotlib Introduction](pandas/matplotlib.ipynb)  \n",
    "- [Visualization Rules](visualization_rules.ipynb)  \n",
    "- Regression  \n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- See an end-to-end data science exercise  \n",
    "- Application of regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, preprocessing, model_selection\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Recidivism\n",
    "\n",
    "Recidivism is the tendency for an individual who has previously committed a crime to commit another crime\n",
    "in the future. One key input to a judge’s sentencing decision is how likely a given convict is to re-offend, or recidivate\n",
    "\n",
    "In an effort to assist the legal system with sentencing guidelines, data scientists have attempted\n",
    "to predict an individual’s risk of recidivism from known observables. Some are concerned that this process\n",
    "may exhibit prejudice, either through biased inputs or through statistical discrimination. For example,\n",
    "\n",
    "1. \n",
    "  <dl style='margin: 20px 0;'>\n",
    "  <dt>Biased inputs: Imagine that a judge often writes harsher sentences to people of a particular</dt>\n",
    "  <dd>\n",
    "  race or gender. If an algorithm is trained to reproduce the sentences of this judge,\n",
    "  the bias will be propagated by the algorithm  \n",
    "  </dd>\n",
    "  \n",
    "  </dl>\n",
    "  \n",
    "1. \n",
    "  <dl style='margin: 20px 0;'>\n",
    "  <dt>Statistical discrimination: Imagine that two variables (say race and income) are correlated, and one of them (say income)</dt>\n",
    "  <dd>\n",
    "  is correlated with the risk of recidivism. If income is unobserved, then an otherwise unbiased\n",
    "  method would discriminate based on race, even if race has nothing to say about recidivism after\n",
    "  controlling for income  \n",
    "  </dd>\n",
    "  \n",
    "  </dl>\n",
    "  \n",
    "\n",
    "\n",
    "This has given rise to serious discussions about the moral obligations data scientists have to\n",
    "those who are affected by their tools. We will not take a stance today on our moral obligations, but\n",
    "we believe this is an important precursor to any statistical work with public policy applications\n",
    "\n",
    "One predictive tool used by various courts in the United States is\n",
    "called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions). We will be\n",
    "following a [Pro Publica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n",
    "article that analyzes the output of COMPAS. The findings of the article include:\n",
    "\n",
    "- Black defendants were often predicted to be at a higher risk of recidivism than they actually were  \n",
    "- White defendants were often predicted to be less risky than they were  \n",
    "- When controlling for prior crimes, future recidivism, age, and gender, black defendants were 45\n",
    "  percent more likely to be assigned higher risk scores than white defendants  \n",
    "- Black defendants were twice as likely as white defendants to be misclassified as being a higher\n",
    "  risk of violent recidivism  \n",
    "- Even when controlling for prior crimes, future recidivism, age, and gender, black defendants were\n",
    "  77 percent more likely to be assigned higher risk scores than white defendants  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The authors of this article filed a public records request with the Broward County Sheriff’s office\n",
    "in Florida. Luckily for us, they did a significant amount of the legwork which is described in this\n",
    "[methodology article](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "\n",
    "We download the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "data_url = \"https://raw.githubusercontent.com/propublica/compas-analysis\"\n",
    "data_url += \"/master/compas-scores-two-years.csv\"\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize some of the variables that we will use\n",
    "\n",
    "- `first`: An individual’s first name  \n",
    "- `last`: An individual’s last name  \n",
    "- `sex`: An individual’s sex  \n",
    "- `age`: An individual’s age  \n",
    "- \n",
    "  <dl style='margin: 20px 0;'>\n",
    "  <dt>`race`: An individual’s race. It takes values of Caucasian, Hispanic, African-American, Native</dt>\n",
    "  <dd>\n",
    "  American, Asian, or Other  \n",
    "  </dd>\n",
    "  \n",
    "  </dl>\n",
    "  \n",
    "- `priors_count`: Number of previous arrests  \n",
    "- `decile_score`: The COMPAS risk score  \n",
    "- `two_year_recid`: Whether the individual had been jailed for a new crime in next two years  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "The first thing we do with our data is to drop any classes without “enough” observations. One of our\n",
    "focuses will be on inter-race differences in scores and recidivism, so we only keep data on races\n",
    "with at least 500 observations in our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "race_count = df.groupby([\"race\"])[\"name\"].count()\n",
    "at_least_500 = list(race_count[race_count > 500].index)\n",
    "print(\"The following race have at least 500 observations:\", at_least_500)\n",
    "df = df.loc[df[\"race\"].isin(at_least_500), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore the remaining data using plots and tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age, Sex, and Race\n",
    "\n",
    "Let’s look at how the dataset is broken down into age, sex, and race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def create_groupcount_barplot(df, group_col, figsize, **kwargs):\n",
    "    \"call df.groupby(group_col), then count number of records and plot\"\n",
    "    counts = df.groupby(group_col)[\"name\"].count().sort_index()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    counts.plot(kind=\"bar\", **kwargs)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "age_cs = [\"Less than 25\", \"25 - 45\", \"Greater than 45\"]\n",
    "df[\"age_cat\"] = pd.Categorical(df[\"age_cat\"], categories=age_cs, ordered=True)\n",
    "fig, ax = create_groupcount_barplot(df, \"age_cat\", (14, 8), color=\"DarkBlue\", rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sex_cs = [\"Female\", \"Male\"]\n",
    "df[\"sex\"] = pd.Categorical(df[\"sex\"], categories=sex_cs, ordered=True)\n",
    "create_groupcount_barplot(df, \"sex\", (6, 8), color=\"DarkBlue\", rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "race_cs = [\"African-American\", \"Caucasian\", \"Hispanic\"]\n",
    "df[\"race\"] = pd.Categorical(df[\"race\"], categories=race_cs, ordered=True)\n",
    "create_groupcount_barplot(df, \"race\", (12, 8), color=\"DarkBlue\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn from this is that the population that we are looking at is mostly between 25-45, male, and\n",
    "is mostly African-American or Caucasian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recidivism\n",
    "\n",
    "We now look into how recidivism is split across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "recid = df.groupby([\"age_cat\", \"sex\", \"race\"])[\"two_year_recid\"].mean().unstack(level=\"race\")\n",
    "recid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table we see that the young have higher recidivism rates than the old, except for among\n",
    "Caucasian females. Also, African-American males are at a particularly high risk of recidivism even\n",
    "as they get older"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Scores\n",
    "\n",
    "Each individual in the dataset was assigned a `decile_score` ranging from 1 to 10\n",
    "\n",
    "This score represents the perceived risk of recidivism with 1 being the lowest risk and 10 being the highest\n",
    "\n",
    "We show a bar plot of all decile scores below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "create_groupcount_barplot(df, \"decile_score\", (12, 8), color=\"DarkBlue\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these scores differ by race?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "dfgb = df.groupby(\"race\")\n",
    "race_count = df.groupby(\"race\")[\"name\"].count()\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(14, 8))\n",
    "\n",
    "for (i, race) in enumerate([\"African-American\", \"Caucasian\", \"Hispanic\"]):\n",
    "    (\n",
    "        (dfgb\n",
    "            .get_group(race)\n",
    "            .groupby(\"decile_score\")[\"name\"].count() / race_count[race]\n",
    "        )\n",
    "        .plot(kind=\"bar\", ax=ax[i], color=\"#353535\")\n",
    "    )\n",
    "    ax[i].set_ylabel(race)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "    # set equal y limit for visual comparison\n",
    "    ax[i].set_ylim(0, 0.32)\n",
    "\n",
    "fig.suptitle(\"Score Frequency by Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Caucasians and Hispanics both see majority of their score distribution on low values,\n",
    "African-Americans are almost equally likely to receive any score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Scores and Recidivism\n",
    "\n",
    "Now we can explore the relationship between the risk score and actual two year recidivism\n",
    "\n",
    "The first measure we look at is the frequency of recidivism by decile score – These numbers\n",
    "tell us what percentage of people assigned a particular risk score committed a new crime within two\n",
    "years of being released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "df.groupby(\"decile_score\")[\"two_year_recid\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also look at the correlation correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "df[[\"decile_score\", \"two_year_recid\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of people committing a new crime is increasing in the risk score and there is a\n",
    "positive correlation (~0.35)\n",
    "\n",
    "This is good news – it means that the score is producing at least some signal about an individual’s recidivism risk\n",
    "\n",
    "One of the key critiques from Pro Publica, though, was that the inaccuracies were nonuniform — that is, the tool was\n",
    "systematically wrong about certain populations\n",
    "\n",
    "Let’s now separate the correlations by race and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "recid_rates = df.pivot_table(index=\"decile_score\", columns=\"race\", values=\"two_year_recid\")\n",
    "\n",
    "recid_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in plotted form,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, sharex=\"all\")\n",
    "\n",
    "for (i, _race) in enumerate([\"African-American\", \"Caucasian\", \"Hispanic\"]):\n",
    "    _rr_vals = recid_rates[_race].values\n",
    "\n",
    "    ax[i].bar(np.arange(1, 11), _rr_vals, color=\"#c60000\")\n",
    "    ax[i].bar(np.arange(1, 11), 1 - _rr_vals, bottom=_rr_vals, color=\"#353535\")\n",
    "    ax[i].set_ylabel(_race)\n",
    "    ax[i].spines[\"left\"].set_visible(False)\n",
    "    ax[i].spines[\"right\"].set_visible(False)\n",
    "    ax[i].spines[\"top\"].set_visible(False)\n",
    "    ax[i].spines[\"bottom\"].set_visible(False)\n",
    "    ax[i].yaxis.tick_right()\n",
    "    ax[i].xaxis.set_ticks_position(\"none\")\n",
    "\n",
    "fig.suptitle(\"Recidivism Rates by Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "In what follows, we will be doing something slightly different than what was done in the Pro Publica\n",
    "article\n",
    "\n",
    "First, we will explore what happens when we try to predict the COMPAS risk scores using the\n",
    "observable data that we have\n",
    "\n",
    "Second, we will use binary probability models to try and predict whether an individual is at risk of\n",
    "recidivism. We will do this first using the COMPAS risk scores, and then afterwards we will try to write our own\n",
    "model based on raw observables, like age, race and sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We would like to use some features that are inherently non-numerical such as sex, age group, and\n",
    "race in our model. Before we can do that we need to encode these string values as numerical values\n",
    "so our machine learning algorithms can understand them – An econometrician would call this,\n",
    "creating dummy variables. `sklearn` can automatically do this for us using `OneHotEncoder`\n",
    "\n",
    "The main idea of how this works is to make one column for each possible value of a categorical\n",
    "variable then we will set just one of these columns equal to a 1 if the observation has that\n",
    "column’s category, and set all other columns to 0.\n",
    "\n",
    "Let’s do an example\n",
    "\n",
    "Imagine we have the array below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sex = np.array([[\"Male\"], [\"Female\"], [\"Male\"], [\"Male\"], [\"Female\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to encode this would be to create the array below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sex_encoded = np.array([\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn` it would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "sex_ohe = ohe.fit_transform(sex)\n",
    "\n",
    "# This should shows 0s!\n",
    "sex_ohe - sex_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this encoding trick below as we create our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting COMPAS Scores\n",
    "\n",
    "First we proceed by creating the `X` and `y` inputs into a manageable format. We encode the categorical\n",
    "variables using the `OneHotEncoder` described above, and then merge that with the non-categorical data. Finally,\n",
    "we split the data into training and validation (test) subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def prep_data(df, continuous_variables, categories, y_var, test_size=0.15):\n",
    "\n",
    "    ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "\n",
    "    y = df[y_var].values\n",
    "    X = np.zeros((y.size, 0))\n",
    "\n",
    "    # Add continuous variables if exist\n",
    "    if len(continuous_variables) > 0:\n",
    "        X = np.hstack([X, df[continuous_variables].values])\n",
    "\n",
    "    if len(categories) > 0:\n",
    "        X = np.hstack([X, ohe.fit_transform(df[categories])])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we proceed, our goal will be to see which variables are most important for predicting the COMPAS\n",
    "scores. As we estimate these models, one of our metrics for success will be mean absolute error\n",
    "(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def fit_and_report_maes(mod, X_train, X_test, y_train, y_test, y_transform=None, y_inv_transform=None):\n",
    "    if y_transform is not None:\n",
    "        mod.fit(X_train, y_transform(y_train))\n",
    "    else:\n",
    "        mod.fit(X_train, y_train)\n",
    "\n",
    "    yhat_train = mod.predict(X_train)\n",
    "    yhat_test = mod.predict(X_test)\n",
    "\n",
    "    if y_transform is not None:\n",
    "        yhat_train = y_inv_transform(yhat_train)\n",
    "        yhat_test = y_inv_transform(yhat_test)\n",
    "\n",
    "    return dict(\n",
    "        mae_train=metrics.mean_absolute_error(y_train, yhat_train),\n",
    "        mae_test=metrics.mean_absolute_error(y_test, yhat_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s begin with a simple linear model which uses just prior arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    df, [\"priors_count\"], [], \"decile_score\"\n",
    ")\n",
    "\n",
    "fit_and_report_maes(linear_model.LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model obtains a MAE of about 2 for both the test data and training data. This means, on\n",
    "average, that our model can predict the COMPAS score (which ranges from 1-10) within about 2 points\n",
    "\n",
    "While the MAE is about 2, it’s often useful to know what the errors on our prediction model look\n",
    "like. Below we create a histogram which shows the distribution of these errors – In our case we\n",
    "take the difference between predicted value and actual value, so a positive value means that we\n",
    "overpredicted the COMPAS score and a negative value means we underpredicted it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "yhat_train = lr_model.predict(X_train)\n",
    "yhat_test = lr_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=\"all\")\n",
    "\n",
    "ax[0].hist(yhat_train - y_train, density=True)\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].hist(yhat_test - y_test, density=True)\n",
    "ax[1].set_title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, there are long left tails of errors meaning there are features of the data that might\n",
    "be missing that would allow us to accurately predict the scores\n",
    "\n",
    "The first thing we might consider investigating is whether there are non-linearities in how the\n",
    "number of priors enters the COMPAS score\n",
    "\n",
    "First we try using polynomial features in our exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    df, [\"priors_count\"], [], \"decile_score\"\n",
    ")\n",
    "\n",
    "# Transform data to quadratic\n",
    "pf = preprocessing.PolynomialFeatures(2, include_bias=False)\n",
    "X_train = pf.fit_transform(X_train)\n",
    "X_test = pf.fit_transform(X_test)\n",
    "\n",
    "fit_and_report_maes(linear_model.LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t see a very significant increase in performance, so we also try using log on the endogenous\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    df, [\"priors_count\"], [], \"decile_score\"\n",
    ")\n",
    "\n",
    "fit_and_report_maes(\n",
    "    linear_model.LinearRegression(), X_train, X_test, y_train, y_test,\n",
    "    y_transform=np.log, y_inv_transform=np.exp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no improvement… The next natural thing is to add more features to our regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    df, [\"priors_count\"], [\"age_cat\", \"race\", \"sex\"], \"decile_score\"\n",
    ")\n",
    "\n",
    "fit_and_report_maes(linear_model.LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By allowing for indicator variables on age, race, and sex, we are able to slightly improve the MAE.\n",
    "The errors also seem to have a less extreme tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    df, [\"priors_count\"], [\"age_cat\", \"race\", \"sex\"], \"decile_score\"\n",
    ")\n",
    "\n",
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "yhat_train = lr_model.predict(X_train)\n",
    "yhat_test = lr_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=\"all\")\n",
    "\n",
    "ax[0].hist(yhat_train - y_train, density=True)\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].hist(yhat_test - y_test, density=True)\n",
    "ax[1].set_title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"priors_count\", \"Less than 25\", \"25-45\", \"Greater than 45\", \"African-American\",\n",
    "    \"Caucasian\", \"Hispanic\", \"Female\", \"Male\"\n",
    "]\n",
    "for (_name, _coef) in zip(names, lr_model.coef_[0, :]):\n",
    "    print(_name, \": \", _coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What stands out to you about these coefficients?\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Can you develop a model that performs better at mimicking their risk scores?\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Probability Models\n",
    "\n",
    "Binary probability models are used to model the probability of an event occurring. The output of\n",
    "this family of model is the probability that an event of interest occurs. With this probability in\n",
    "hand, the researcher chooses an acceptable cutoff (perhaps 0.5) above which the event is predicted\n",
    "to occur\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Binary probability models can be thought of as a special case of\n",
    "classification. In classification we are given a set of features and asked\n",
    "to predict one of a finite number of discrete labels. We will learn more\n",
    "about classification in an upcoming lecture!\n",
    "\n",
    "In our example, we will be interested in how the COMPAS scores do at predicting recidivism and how\n",
    "their ability to predict depends on race or sex\n",
    "\n",
    "To assist us in evaluating the performance of various models we will use a new\n",
    "metric called the *confusion matrix*. Scikit-learn knows how to compute this\n",
    "metric, and also provides a good description of what is computed. Let’s see\n",
    "what they have to say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "help(metrics.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def fit_and_report_cm(mod, X_train, X_test, y_train, y_test):\n",
    "    mod.fit(X_train, y_train)\n",
    "    return dict(\n",
    "        cm_train=metrics.confusion_matrix(y_train, mod.predict(X_train)),\n",
    "        cm_test=metrics.confusion_matrix(y_test, mod.predict(X_test))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by using logistic regression using only `decile_score` as a feature and then examine\n",
    "how the confusion matrices differ by race and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "    \"overall\", \"African-American\", \"Caucasian\", \"Hispanic\", \"Female\", \"Male\"\n",
    "]\n",
    "\n",
    "ind = [\n",
    "    \"Percent_of_NoRecid_given_LowRisk\", \"Percent_of_Recid_given_LowRisk\",\n",
    "    \"Percent_of_NoRecid_given_HighRisk\", \"Percent_of_Recid_given_HighRisk\"\n",
    "]\n",
    "output = pd.DataFrame(index=ind, columns=groups)\n",
    "\n",
    "for group in groups:\n",
    "    if group in [\"African-American\", \"Caucasian\", \"Hispanic\"]:\n",
    "        df_subset = df.query(\"race == @group\")\n",
    "    elif group in [\"Female\", \"Male\"]:\n",
    "        df_subset = df.query(\"sex == @group\")\n",
    "    else:\n",
    "        df_subset = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = prep_data(\n",
    "        df_subset, [\"decile_score\"], [], \"two_year_recid\", test_size=0.25\n",
    "    )\n",
    "\n",
    "    cm = fit_and_report_cm(\n",
    "        linear_model.LogisticRegression(), X_train, X_test, y_train, y_test\n",
    "    )[\"cm_test\"]\n",
    "\n",
    "    # Compute fraction for which the guess is correct\n",
    "    total_lowrisk = cm[:, 0].sum()\n",
    "    total_highrisk = cm[:, 1].sum()\n",
    "    vals = np.array([\n",
    "        cm[0, 0] / total_lowrisk, cm[1, 0] / total_lowrisk,\n",
    "        cm[0, 1] / total_highrisk, cm[1, 1] / total_highrisk\n",
    "    ])\n",
    "    output.loc[:, group] = vals\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output` contains information on the fraction of true negatives, false negatives, false positives,\n",
    "and true positives. What do you see?"
   ]
  }
 ],
 "metadata": {
  "filename": "recidivism.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Case Study: Recidivism"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}